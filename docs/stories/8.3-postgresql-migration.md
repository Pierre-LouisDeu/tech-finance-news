# Story 8.3: PostgreSQL Migration & Production Cron

## Epic
Epic 8: Daily Digest & Production Hardening

## Description
Migrer la base de données SQLite vers PostgreSQL pour une architecture production robuste, et configurer le scheduling via crontab serveur au lieu du scheduler Node.js interne.

## User Story
**En tant qu'** opérateur de production
**Je veux** une base PostgreSQL externe et un cron système
**Afin d'** avoir une architecture fiable, maintenable et standard

## Acceptance Criteria

### PostgreSQL Migration
- [ ] Remplacer `better-sqlite3` par le package `pg` (node-postgres)
- [ ] Adapter toutes les queries SQL à la syntaxe PostgreSQL
- [ ] Implémenter un pool de connexions
- [ ] Variable d'environnement `DATABASE_URL` pour la connexion
- [ ] Script de migration/initialisation du schema
- [ ] Toutes les tables migrées : `articles`, `processing_log`, `summaries`, `notion_sync`, `daily_briefings`

### Production Cron
- [ ] Script shell `scripts/run-pipeline.sh` pour lancer le pipeline
- [ ] Documentation crontab dans `docs/deployment.md`
- [ ] Supprimer la dépendance `node-cron`
- [ ] Retirer le mode `--scheduled` de l'application
- [ ] L'app s'exécute une fois et s'arrête (mode one-shot uniquement)

### Configuration
- [ ] `.env.example` mis à jour avec `DATABASE_URL`
- [ ] Dockerfile adapté (sans SQLite dependencies)
- [ ] docker-compose.yml avec service PostgreSQL pour dev local

## Technical Notes

### Dépendances

```bash
# Supprimer
npm uninstall better-sqlite3 @types/better-sqlite3 node-cron @types/node-cron

# Ajouter
npm install pg
npm install -D @types/pg
```

### Connection Pool

```typescript
// src/db/index.ts
import { Pool } from 'pg';

const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 10,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

export async function query<T>(text: string, params?: unknown[]): Promise<T[]> {
  const result = await pool.query(text, params);
  return result.rows as T[];
}

export async function getClient() {
  return pool.connect();
}
```

### Schema PostgreSQL

```sql
-- src/db/schema.sql

CREATE TABLE IF NOT EXISTS articles (
  id TEXT PRIMARY KEY,
  title TEXT NOT NULL,
  url TEXT NOT NULL UNIQUE,
  content TEXT,
  published_at TIMESTAMPTZ NOT NULL,
  source TEXT NOT NULL DEFAULT 'abcbourse',
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS processing_log (
  id SERIAL PRIMARY KEY,
  article_id TEXT NOT NULL REFERENCES articles(id) ON DELETE CASCADE,
  stage TEXT NOT NULL CHECK (stage IN ('scraped', 'filtered', 'summarized', 'pushed')),
  status TEXT NOT NULL CHECK (status IN ('success', 'failed', 'skipped')),
  error_message TEXT,
  processed_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS summaries (
  article_id TEXT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
  short_summary TEXT NOT NULL,
  detailed_summary TEXT,
  tokens_used INTEGER,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS notion_sync (
  article_id TEXT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
  notion_page_id TEXT NOT NULL,
  synced_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE IF NOT EXISTS daily_briefings (
  date DATE PRIMARY KEY,
  article_count INTEGER NOT NULL DEFAULT 0,
  global_summary TEXT NOT NULL,
  notion_page_id TEXT,
  created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes
CREATE INDEX IF NOT EXISTS idx_articles_published_at ON articles(published_at DESC);
CREATE INDEX IF NOT EXISTS idx_articles_source ON articles(source);
CREATE INDEX IF NOT EXISTS idx_processing_log_article ON processing_log(article_id);
CREATE INDEX IF NOT EXISTS idx_processing_log_stage_status ON processing_log(stage, status);
CREATE INDEX IF NOT EXISTS idx_daily_briefings_date ON daily_briefings(date DESC);
```

### Différences SQL SQLite → PostgreSQL

| SQLite | PostgreSQL |
|--------|------------|
| `datetime('now')` | `NOW()` |
| `date('now', 'localtime')` | `CURRENT_DATE` |
| `TEXT` pour dates | `TIMESTAMPTZ` |
| `INTEGER PRIMARY KEY AUTOINCREMENT` | `SERIAL PRIMARY KEY` |
| `?` placeholders | `$1, $2, $3` placeholders |
| Synchrone | Asynchrone (async/await) |

### Script run-pipeline.sh

```bash
#!/bin/bash
# scripts/run-pipeline.sh
# Execute pipeline via Docker

set -e

CONTAINER_NAME="tech-finance-pipeline"
IMAGE_NAME="tech-finance-news:latest"
ENV_FILE="/opt/tech-finance/.env"
LOG_FILE="/var/log/tech-finance/pipeline.log"

echo "[$(date -Iseconds)] Starting pipeline..." >> "$LOG_FILE"

docker run --rm \
  --name "$CONTAINER_NAME" \
  --env-file "$ENV_FILE" \
  --network host \
  "$IMAGE_NAME" \
  node dist/index.js --run \
  >> "$LOG_FILE" 2>&1

echo "[$(date -Iseconds)] Pipeline completed" >> "$LOG_FILE"
```

### Crontab Configuration

```bash
# crontab -e
# Tech Finance News Pipeline - Mon-Fri at 8h, 11h, 14h, 17h, 20h (Europe/Paris)
SHELL=/bin/bash
TZ=Europe/Paris

0 8,11,14,17,20 * * 1-5 /opt/tech-finance/scripts/run-pipeline.sh
```

### DATABASE_URL Format

```
postgresql://user:password@localhost:5432/tech_finance_news
```

Pour Dokploy avec PostgreSQL service :
```
postgresql://dokploy:password@postgres:5432/tech_finance_news
```

### docker-compose.yml (dev local)

```yaml
version: '3.8'

services:
  app:
    build: .
    environment:
      - DATABASE_URL=postgresql://techfinance:devpassword@db:5432/tech_finance_news
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - NOTION_API_KEY=${NOTION_API_KEY}
      - NOTION_DATABASE_ID=${NOTION_DATABASE_ID}
    depends_on:
      db:
        condition: service_healthy
    command: ["node", "dist/index.js", "--run"]

  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: techfinance
      POSTGRES_PASSWORD: devpassword
      POSTGRES_DB: tech_finance_news
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U techfinance -d tech_finance_news"]
      interval: 5s
      timeout: 5s
      retries: 5

volumes:
  pgdata:
```

## Implementation Tasks

### Phase 1: PostgreSQL Migration
1. [ ] Installer `pg` et supprimer `better-sqlite3`
2. [ ] Créer `src/db/schema.sql` avec syntaxe PostgreSQL
3. [ ] Réécrire `src/db/index.ts` avec Pool de connexions
4. [ ] Migrer `src/db/queries.ts` (async + placeholders $1,$2)
5. [ ] Adapter `src/digest/briefing.ts` pour PostgreSQL
6. [ ] Mettre à jour `src/config/env.ts` avec `DATABASE_URL`

### Phase 2: Simplification App
7. [ ] Supprimer `node-cron` et `src/scheduler.ts`
8. [ ] Simplifier `src/index.ts` (mode one-shot uniquement)
9. [ ] Créer `scripts/run-pipeline.sh`

### Phase 3: Configuration
10. [ ] Mettre à jour `.env.example`
11. [ ] Mettre à jour `docker-compose.yml`
12. [ ] Mettre à jour `Dockerfile` (retirer deps SQLite)
13. [ ] Documenter setup PostgreSQL + crontab dans `docs/deployment.md`

### Phase 4: Tests
14. [ ] Tester en local avec docker-compose
15. [ ] Vérifier toutes les opérations CRUD
16. [ ] Tester le script cron manuellement

## Dependencies
- Story 8.2 (Daily Briefing Section) - COMPLETED

## Out of Scope
- Migration des données existantes SQLite → PostgreSQL (fresh start)
- Interface d'administration PostgreSQL
- Réplication / haute disponibilité

## Estimation
Points: 8 (story complexe, refactoring significatif)

## Risks & Mitigations

| Risque | Mitigation |
|--------|------------|
| Queries SQL incompatibles | Tests unitaires sur chaque query |
| Connexion pool issues | Health checks, timeouts configurés |
| Perte de données migration | Pas de migration, fresh start en prod |
| Cron ne s'exécute pas | Logs détaillés, monitoring cron |

## Definition of Done
- [ ] Toutes les queries fonctionnent avec PostgreSQL
- [ ] App démarre et se connecte à PostgreSQL
- [ ] Pipeline complet s'exécute avec succès
- [ ] docker-compose up fonctionne en local
- [ ] Script cron documenté et testé
- [ ] Aucune référence à SQLite dans le code
- [ ] `node-cron` supprimé des dépendances
